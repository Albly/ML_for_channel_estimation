{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alex/GitHub/ML_for_channel_estimation\n",
      "\u001b[0m\u001b[01;34mch_est_net\u001b[0m/        lista_beam_net.pt  output.svg        torchvix-sample\n",
      "config_beams.yaml  model_image        \u001b[01;34m__pycache__\u001b[0m/      torchvix-sample.png\n",
      "config.yaml        model_image.png    README.md         zero_150.pt\n",
      "data.h5            \u001b[01;34mNotebooks\u001b[0m/         requirements.txt  zero.pt\n",
      "\u001b[01;34mDMRS_signal\u001b[0m/       one.pt             \u001b[01;34mresults\u001b[0m/\n",
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls \n",
    "\n",
    "from ch_est_net.preloading import *\n",
    "from ch_est_net.utils import *\n",
    "\n",
    "from torch.fft import fft, ifft, fftshift, ifftshift\n",
    "from ch_est_net import activation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "\n",
      "Preload data: OK\n"
     ]
    }
   ],
   "source": [
    "cfg, dataL, dataS, device, deviceType, dtype, file, ml_default, onePilotFolder, path ,preload, print_function, scen0, Scenario = crazy_preloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(snr_range, ind_range, seed):\n",
    "    '''\n",
    "    For files ind_range add noise with snr_range and stack all realizations to single array\n",
    "    --------------------------------------------------------------------------\n",
    "    RETURNS:\n",
    "    pilot_batch -> [Realizations ; Antennas ; Subc ; Re/Im ]\n",
    "    data_batch  -> [Realizations ; Antennas ; Subc ; 12? ; Re/Im]\n",
    "    pilot_batch -> [Realizations]\n",
    "    --------------------------------------------------------------------------\n",
    "    Realizations = len(snr_range)*len(ind_range) \n",
    "    '''\n",
    "\n",
    "    pilot_batch = torch.tensor(())\n",
    "    pilot_noisy_batch = torch.tensor(())\n",
    "    data_batch = torch.tensor(())\n",
    "    noise_P_batch = torch.tensor(())\n",
    "\n",
    "    for snr in snr_range:\n",
    "        for ind in ind_range:\n",
    "            h_pilot, h_data = data_load(scen0, dtype = dtype, onePilotFolder = onePilotFolder,dataL=dataL, ind = ind) \n",
    "                                                  # load data\n",
    "            h_pilot_noisy, _ = add_noise(h_pilot, SNR = snr,scen=scen0, dtype=dtype, seed = seed)                             # add noise to pilots\n",
    "            h_data_noisy, data_noise_power = add_noise_data(h_data, SNR = snr, dtype = dtype, seed = seed)     # add noise to data\n",
    "\n",
    "            h_pilot = h_pilot.mean(dim = 2).unsqueeze(dim = 0)                                  # mean over pilots dim and add aditional dim\n",
    "            h_pilot_noisy = h_pilot_noisy.mean(dim = 2).unsqueeze(dim = 0)\n",
    "\n",
    "            h_data_noisy = h_data_noisy.unsqueeze(dim = 0)                                      # add additional dim for\n",
    "            data_noise_power = data_noise_power.unsqueeze(dim =0)\n",
    "\n",
    "            pilot_batch = torch.cat((pilot_batch, h_pilot), dim = 0)\n",
    "\n",
    "            pilot_noisy_batch = torch.cat((pilot_noisy_batch, h_pilot_noisy), dim = 0)\n",
    "            data_batch = torch.cat((data_batch, h_data_noisy), dim = 0)\n",
    "            noise_P_batch = torch.cat((noise_P_batch, data_noise_power), dim = 0)\n",
    "\n",
    "    return pilot_batch, pilot_noisy_batch, data_batch, noise_P_batch\n",
    "\n",
    "\n",
    "\n",
    "snr_range = [-12, -10, -8 ,-5 , -2, 5, 10, 30]\n",
    "x_batch, u_batch, _, _ = get_batch(snr_range = snr_range,\n",
    "                                            ind_range = range(1,140,5),\n",
    "                                            seed = 312)\n",
    "\n",
    "\n",
    "\n",
    "x_test, u_test, _, _ = get_batch(snr_range = [-12, -10, -8 ,-5 , -2],\n",
    "                                              ind_range = range(2, 141, 5),\n",
    "                                              seed = 2)\n",
    "\n",
    "\n",
    "x_batch = x_batch[:,:,:,0] + 1j*x_batch[:,:,:,1]\n",
    "u_batch = u_batch[:,:,:,0] + 1j*u_batch[:,:,:,1]\n",
    "\n",
    "x_test = x_test[:,:,:,0] + 1j*x_test[:,:,:,1]\n",
    "u_test = u_test[:,:,:,0] + 1j*u_test[:,:,:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_time_transform(x):\n",
    "    if len(x.shape)==2:\n",
    "        x = torch.unsqueeze(x, dim = 0)\n",
    "\n",
    "    x_a_t = ifft(x , n = 512, dim = 2, norm = 'ortho')\n",
    "    x_a_t = torch.roll(x_a_t, shifts = 206, dims= 2)\n",
    "    x_b_t = fft(x_a_t, n = 512, dim = 1, norm = 'ortho')\n",
    "\n",
    "    return torch.squeeze(x_b_t, dim = 0)    \n",
    "\n",
    "def ant_freq_transform(x):\n",
    "    if len(x.shape)==2:\n",
    "        x = torch.unsqueeze(x, dim = 0)\n",
    "    \n",
    "    x = torch.roll(x, shifts= -206, dims =2)\n",
    "    x_a_t = ifft(x, n = 512, dim = 1, norm = 'ortho')\n",
    "    x_a_t = x_a_t[:,:64,:]\n",
    "    x_a_f = fft(x_a_t, n = 512, dim = 2, norm ='ortho')\n",
    "    x_a_f = x_a_f[:,:,:48]\n",
    "    \n",
    "    return torch.squeeze(x_a_f, dim = 0)\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "def MSE(x_real, x_hat):\n",
    "    '''Mean squared error generalized for complex values'''\n",
    "\n",
    "    assert x_real.shape == x_hat.shape, 'Sizes of both values must be the same, but got {0} and {1} instead'.format(x_real.shape, x_hat.shape)\n",
    "    \n",
    "    if torch.is_complex(x_real):\n",
    "        mse = torch.sum(torch.abs(x_real-x_hat)**2)/torch.numel(x_real)\n",
    "\n",
    "    else:\n",
    "        mse = mse_loss(x_real, x_hat)\n",
    "\n",
    "    return mse\n",
    "\n",
    "x_batch_spec = beam_time_transform(x_batch)\n",
    "u_batch_spec = beam_time_transform(u_batch)\n",
    "x_test_spec = beam_time_transform(x_test)\n",
    "u_test_spec = beam_time_transform(u_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_scaller(in_data):\n",
    "    out_data = torch.zeros_like(in_data)\n",
    "    for idx in range(in_data.shape[0]):\n",
    "        out_data[idx] = in_data[idx] - torch.mean(in_data[idx])\n",
    "        out_data[idx] = in_data[idx] / torch.std(in_data[idx], unbiased = False)\n",
    "    return out_data \n",
    "\n",
    "\n",
    "def re_im_mean_scaller(in_data):\n",
    "    out_data_re = torch.zeros_like(in_data)\n",
    "    out_data_im = torch.zeros_like(in_data)\n",
    "    \n",
    "    for idx in range(in_data.shape[0]):\n",
    "        out_data_re[idx] = in_data[idx].real - torch.mean(in_data[idx].real)\n",
    "        out_data_re[idx] = in_data[idx].real / torch.std(in_data[idx].real, unbiased = False)\n",
    "        \n",
    "        out_data_im[idx] = in_data[idx].imag - torch.mean(in_data[idx].imag)\n",
    "        out_data_im[idx] = in_data[idx].imag / torch.std(in_data[idx].imag, unbiased = False)\n",
    "\n",
    "\n",
    "    return out_data_re + 1j*out_data_im \n",
    "\n",
    "\n",
    "\n",
    "def noise_mean_scaller(in_data):\n",
    "    out_data = torch.zeros_like(in_data)\n",
    "    in_data = torch.roll(in_data, shifts = 150, dims = 1)\n",
    "    noise_data = in_data[:,10:150,:]\n",
    "\n",
    "    for idx in range(in_data.shape[0]):\n",
    "        out_data[idx] = in_data[idx] - torch.mean(noise_data[idx])\n",
    "        out_data[idx] = in_data[idx] / torch.std(noise_data[idx], unbiased = False)\n",
    "    return out_data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape [SNR, file, beams, time]\n",
    "x_batch_spec_ = torch.reshape(x_batch_spec, (len(snr_range),-1,512,512))\n",
    "u_batch_spec_ = torch.reshape(u_batch_spec, (len(snr_range),-1,512,512))\n",
    "x_batch_spec_scaled = torch.reshape(mean_scaller(x_batch_spec), (len(snr_range),-1,512,512))\n",
    "u_batch_spec_scaled = torch.reshape(mean_scaller(u_batch_spec), (len(snr_range),-1,512,512))\n",
    "x_batch_spec_noise_scaled = torch.reshape(noise_mean_scaller(x_batch_spec), (len(snr_range),-1,512,512))\n",
    "u_batch_spec_noise_scaled = torch.reshape(noise_mean_scaller(u_batch_spec), (len(snr_range),-1,512,512))\n",
    "x_batch_spec_re_im = torch.reshape(re_im_mean_scaller(x_batch_spec), (len(snr_range),-1,512,512))\n",
    "u_batch_spec_re_im = torch.reshape(re_im_mean_scaller(u_batch_spec), (len(snr_range),-1,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 2019 1127 Modified   by S. Liu\n",
    "# 2020 0321 ReModified by S. Liu\n",
    "# 2022 0117 Re S. Liu\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from scipy import io\n",
    "from utils import cov\n",
    "\n",
    "\n",
    "class ComplexBN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True):\n",
    "        super(ComplexBN, self).__init__()\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.eps = eps\n",
    "        self.num_features = num_features\n",
    "        # self.batchNorm2dF = torch.nn.BatchNorm2d(num_features,\n",
    "        #                                          affine=affine).to(self.device)\n",
    "\n",
    "    def forward(self, x):  # shpae of x : [batch,2,channel,axis1,axis2]\n",
    "        # divide dim=1 to 2 parts -> real and imag\n",
    "        # real/imag = [batch, channel, axis1, axis2]\n",
    "\n",
    "        real = x[:, 0]\n",
    "        imag = x[:, 1]\n",
    "\n",
    "        realVec = torch.flatten(real)\n",
    "        imagVec = torch.flatten(imag)\n",
    "        re_im_stack = torch.stack((realVec, imagVec), dim=1)\n",
    "        covMat = cov(re_im_stack)\n",
    "        e, v = torch.symeig(covMat, True)\n",
    "        covMat_sq2 = torch.mm(torch.mm(v, torch.diag(torch.pow(e, -0.5))),\n",
    "                              v.t())\n",
    "        data = torch.stack((realVec - real.mean(), imagVec - imag.mean()),\n",
    "                           dim=1).t()\n",
    "        whitenData = torch.mm(covMat_sq2, data)\n",
    "        real_data = whitenData[0, :].reshape(real.shape[0], real.shape[1],\n",
    "                                             real.shape[2], real.shape[3])\n",
    "        imag_data = whitenData[1, :].reshape(real.shape[0], real.shape[1],\n",
    "                                             real.shape[2], real.shape[3])\n",
    "        output = torch.stack((real_data, imag_data), dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ComplexConv2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(ComplexConv2D, self).__init__()\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.padding = padding\n",
    "\n",
    "        # Model components\n",
    "        # define complex conv\n",
    "        self.conv_re = torch.nn.Conv2d(in_channels,\n",
    "                                       out_channels,\n",
    "                                       kernel_size,\n",
    "                                       stride=stride,\n",
    "                                       padding=padding,\n",
    "                                       dilation=dilation,\n",
    "                                       groups=groups,\n",
    "                                       bias=bias).to(self.device)\n",
    "        self.conv_im = torch.nn.Conv2d(in_channels,\n",
    "                                       out_channels,\n",
    "                                       kernel_size,\n",
    "                                       stride=stride,\n",
    "                                       padding=padding,\n",
    "                                       dilation=dilation,\n",
    "                                       groups=groups,\n",
    "                                       bias=bias).to(self.device)\n",
    "        self.weight1 = self.conv_re.weight\n",
    "        self.weight2 = self.conv_im.weight\n",
    "        self.bias1 = self.conv_re.bias\n",
    "        self.bias2 = self.conv_im.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        paddingF = torch.nn.ZeroPad2d(1)\n",
    "        # print(x.shape)\n",
    "        r = paddingF(x[:, 0])  # NCHW\n",
    "        i = paddingF(x[:, 1])\n",
    "        # print(r.shape)\n",
    "        # New 20191102\n",
    "        r[:, :, 0, :], i[:, :, 0, :] = r[:, :, -2, :], i[:, :, -2, :]\n",
    "        r[:, :, -1, :], i[:, :, -1, :] = r[:, :, 1, :], i[:, :, 1, :]\n",
    "        r[:, :, :, 0], i[:, :, :, 0] = r[:, :, :, 2], i[:, :, :, 2]\n",
    "        r[:, :, :, -1], i[:, :, :, -1] = r[:, :, :, 1], i[:, :, :, 1]\n",
    "        # NEW END\n",
    "        real = self.conv_re(r) - self.conv_im(i)\n",
    "        # print(real.shape)\n",
    "        imaginary = self.conv_re(i) + self.conv_im(r)\n",
    "        # stack real and imag part together @ dim=1\n",
    "        output = torch.stack((real, imaginary), dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ComplexReLU(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inplace=False):\n",
    "        super(ComplexReLU, self).__init__()\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.relu_re = torch.nn.ReLU(inplace=inplace).to(self.device)\n",
    "        self.relu_im = torch.nn.ReLU(inplace=inplace).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.stack(\n",
    "            (self.relu_re(x[:, 0]), self.relu_im(x[:, 1])), dim=1).to(self.device)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ComplexDnCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 depth=17,\n",
    "                 n_channels=64,\n",
    "                 image_channels=1,\n",
    "                 use_bnorm=True,\n",
    "                 kernel_size=3):\n",
    "        super(ComplexDnCNN, self).__init__()\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # kernel_size = 3\n",
    "        padding = 0\n",
    "        layers = []\n",
    "        # 1. Conv2d and ReLU\n",
    "        layers.append(\n",
    "            ComplexConv2D(in_channels=image_channels,\n",
    "                          out_channels=n_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          padding=padding,\n",
    "                          bias=True))\n",
    "        layers.append(ComplexReLU(inplace=False))\n",
    "        # 2. 15 * (Conv2d + BN + ReLU)\n",
    "        for _ in range(depth - 2):\n",
    "            layers.append(\n",
    "                ComplexConv2D(in_channels=n_channels,\n",
    "                              out_channels=n_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=padding,\n",
    "                              bias=False))\n",
    "            '''layers.append(torch.nn.BatchNorm2d(\n",
    "                n_channels, eps=0.0001, momentum=0.95).to(device=self.device))'''\n",
    "            layers.append(ComplexBN(n_channels, eps=0.0001, momentum=0.95))\n",
    "            layers.append(ComplexReLU(inplace=False))\n",
    "        # 3. conv2d\n",
    "        layers.append(\n",
    "            ComplexConv2D(in_channels=n_channels,\n",
    "                          out_channels=image_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          padding=padding,\n",
    "                          bias=False))\n",
    "        self.dncnn = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        out = self.dncnn(x)\n",
    "        return y - out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, channels, num_of_layers = 17):\n",
    "        super(DnCNN, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        features = 64\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(num_of_layers-2):\n",
    "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(features))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.dncnn(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "def MSE(x_real, x_hat):\n",
    "    '''Mean squared error generalized for complex values'''\n",
    "\n",
    "    assert x_real.shape == x_hat.shape, 'Sizes of both values must be the same, but got {0} and {1} instead'.format(x_real.shape, x_hat.shape)\n",
    "    if torch.is_complex(x_real):\n",
    "        mse = torch.sum(torch.abs(x_real-x_hat)**2)/torch.numel(x_real)\n",
    "\n",
    "    else:\n",
    "        mse = mse_loss(x_real, x_hat)\n",
    "\n",
    "    return mse\n",
    "\n",
    "def amplitude_MSE(x_real, x_hat):\n",
    "    assert x_real.shape == x_hat.shape, 'Sizes of both values must be the same, but got {0} and {1} instead'.format(x_real.shape, x_hat.shape)\n",
    "    return mse_loss(abs(x_real), abs(x_hat))\n",
    "\n",
    "def phase_MSE(x_real,x_hat):\n",
    "    assert x_real.shape == x_hat.shape, 'Sizes of both values must be the same, but got {0} and {1} instead'.format(x_real.shape, x_hat.shape)\n",
    "    return mse_loss(torch.angle(x_real), torch.angle(x_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dncnn = DnCNN(1) \n",
    "optim = torch.optim.Adam(dncnn.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer= optim,\n",
    "    mode = 'min',\n",
    "    factor = 0.1,\n",
    "    patience = 10,\n",
    "    threshold = 1e-3,\n",
    "    threshold_mode = 'rel',\n",
    "    cooldown = 10,\n",
    "    min_lr= 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist, test_loss_hist = [], []\n",
    "train_am_hist, test_am_hist = [], []\n",
    "train_phase_hist, test_phase_hist = [], []\n",
    "\n",
    "normalize = False\n",
    "\n",
    "def abs_norm(data):\n",
    "    rho = abs(data)\n",
    "    mu = torch.mean(rho)\n",
    "    psi = torch.std(rho)\n",
    "    \n",
    "    data = data - (mu/2 +1j*mu/2)\n",
    "    data = data / psi\n",
    "\n",
    "    return data, mu, psi \n",
    "\n",
    "def abs_denorm(data, mu, psi):\n",
    "    data = data * psi\n",
    "    data = data + (mu/2 + 1j*mu/2)\n",
    "    return data\n",
    "\n",
    "for epoch in range(1000):\n",
    "    dncnn.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    train_loss, train_loss_am, train_loss_phase = 0,0,0\n",
    "\n",
    "    for idx in range(x_batch_spec.shape[0]):\n",
    "        u = torch.unsqueeze(torch.unsqueeze(u_batch_spec[idx],0),0)\n",
    "        x = torch.unsqueeze(torch.unsqueeze(x_batch_spec[idx],0),0)\n",
    "\n",
    "        mean_u ,std_u = 0,0\n",
    "\n",
    "        if normalize:\n",
    "            u, mean_u, std_u = abs_norm(u)\n",
    "        \n",
    "        noisy_power = abs(u)\n",
    "        signal_power = abs(x)\n",
    "        noise_power = noisy_power - signal_power \n",
    "\n",
    "        x_hat_noise = dncnn(noisy_power)\n",
    "        x_hat_mask = torch.sigmoid(noisy_power-x_hat_noise)\n",
    "        x_hat = u*x_hat_noise \n",
    "        \n",
    "        if normalize:\n",
    "            x_hat = abs_denorm(x_hat, mean_u, std_u)\n",
    "        \n",
    "        train_loss += MSE(x_hat, x)\n",
    "        train_loss_am += amplitude_MSE(x_hat, x)\n",
    "        train_loss_phase += phase_MSE(x_hat, x)\n",
    "\n",
    "    train_loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    train_loss_hist.append(train_loss.item())\n",
    "    train_am_hist.append(train_loss_am.item())\n",
    "    train_phase_hist.append(train_loss_phase.item())\n",
    "\n",
    "    # dncnn.eval()\n",
    "    # test_loss, test_loss_am , test_loss_phase = 0,0,0\n",
    "    # with torch.no_grad():\n",
    "    #     for idx in range(x_test_spec.shape[0]):\n",
    "    #         u = torch.unsqueeze(torch.unsqueeze(u_test_spec[idx],0),0)\n",
    "    #         x = torch.unsqueeze(torch.unsqueeze(x_test_spec[idx],0),0)\n",
    "\n",
    "    #         mean_u ,std_u = 0,0\n",
    "\n",
    "    #         if normalize:\n",
    "    #             u, mean_u, std_u = abs_norm(u)\n",
    "                \n",
    "\n",
    "    #         noisy_power = abs(u)\n",
    "    #         signal_power = abs(x)\n",
    "    #         noise_power = noisy_power - signal_power \n",
    "            \n",
    "    #         x_hat_noise = dncnn(noisy_power)\n",
    "    #         x_hat_mask = torch.sigmoid(noisy_power-x_hat_noise)\n",
    "    #         x_hat = u*x_hat_noise \n",
    "\n",
    "            \n",
    "    #         if normalize:\n",
    "    #             x_hat = abs_denorm(x_hat, mean_u, std_u)\n",
    "\n",
    "    #         test_loss += MSE(x_hat, x)\n",
    "    #         test_loss_am += amplitude_MSE(x_hat,x)\n",
    "    #         test_loss_phase += phase_MSE(x_hat,x)\n",
    "\n",
    "    #     test_loss.append(test_loss.item())\n",
    "    #     test_loss_am.append(test_loss_am.item())\n",
    "    #     test_loss_phase.append(test_loss_phase.item())\n",
    "    #     scheduler.step(test_loss.item())\n",
    "    \n",
    "    # if scheduler.optimizer.param_groups[0]['lr'] < 1e-4:\n",
    "    #     print(\"Min lr has been exceded\")\n",
    "    #     break\n",
    "\n",
    "    print(\"Epoch:{} | Train Loss: {} \".format(epoch, train_loss.item))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
